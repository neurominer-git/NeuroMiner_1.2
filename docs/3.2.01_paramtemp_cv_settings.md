(3.2.01_paramtemp_cv_settings)=
# Cross-validation settings

Cross-validation is a fundamental core of machine learning analysis
because it facilitates out-of-sample generalization and parameter tuning
(see [wiki](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).
NeuroMiner has been built to flexibly create custom cross-validation
schemes depending on your data problem. NeuroMiner has been built around
performing repeated, nested, cross-validation, which is a very robust
method that increases the likelihood of generalization.

The cross-validation works by first defining the following settings:

```{figure} Images/NM_paramtemp_cv_init.png
---
name: fig:3.2.01_cv_menu
alt: neurominer_cv_settings
---
```
NeuroMiner cross-validation framework setup
---

(3.2.01_cv_framework)=
## Select cross-validation framework

This option allows the user to select either k-fold cross-validation
scheme (including leave-one-out) or a leave-group-out analysis with the
following options:

```{figure} Images/NM_paramtemp_cvtypes.png
---
name: fig:3.2.01_cv_types
alt: neurominer_cv_types
---
```
NeuroMiner has been built around the gold standard of repeated, nested cross-validation [(Filzmoser et al.,
2009)](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) and therefore the default options are reflective of this. For a description of cross-validation, you could check-out the supplementary material of this publication: [Koutsouleris et al.,
2016](http://thelancet.com/journals/lanpsy/article/PIIS2215-0366(16)30171-7/fulltext).
In brief, leave-one-out cross-validation and k-fold cross-validation risk overfitting (i.e., your models not generalizing past your sample) in the context of (hyper)parameter optimisation (e.g., optimizing C parameters) or the optimization of feature selection based on the predictive accuracy on the held-out dataset (e.g., using wrappers discussed in [ensemble learning]{3.2.05_paramtemp_ensemble_generation_strategies}). Nested cross-validation mitigates against overfitting and provides models that are more likely to generalize – which is the central aim of machine learning and science generally.

### Nested cross-validation in NeuroMiner

```{figure} Images/nested_cv.png
---
name: fig:3.2.01_nested_cv
alt: neurominer nested cross validation
---
Nested cross validation
```

A depiction of nested cross-validation is represented in {numref}`fig:3.2.01_nested_cv`. In NeuroMiner the outer cross-validation cycle of a nested cross-validation scheme is designated as **CV2** and the inner cross-validation folds are designated as **CV1**. Models are trained in the CV1 cycle and then the best performing models are applied to the CV2 data. This separation of CV2 and CV1 data avoids overfitting.

First the data is split into k number of folds in the outer CV2 set and a CV2 test set is held-out. The rest of the data goes into the ’nest’ where it is again subdivided into k-folds, a test fold is held-out, and the rest of the data is used for training. Models associated with each parameter combination (e.g., C or epsilon) are created in the CV1 training data and applied to the CV1 test data (as shown in {numref}`fig:3.2.01_nested_cv`, **A**). The accuracy is assessed across all parameter combinations for all CV1 models. The optimal parameter combination is chosen based on a criteria set by the user. For each CV1 fold, the training and test data, which has been preprocessed, is retrained with the winning parameter or parameter combination. The models are then applied to the held-out CV2 test data (as shown in {numref}`fig:3.2.01_nested_cv`, **B**).

In cases where there are many (hyper)parameter combinations or when a variable selection procedure is employed, then the user can choose to keep a percentage of top performing models for each fold, instead of only the single optimum model. This is further discussed in [learning algorithm parameters]{3.2.04_paramtemp_learning_algorithm_parameters}.

Let's look at the other cross-validation frameworks included in NeuroMiner:

:::{admonition} Cross-validation frameworks in NeuroMiner
**1 | Pooled**
Outer and inner cross-validation folds will be automatically and randomly defined.

**2 | Outer Leave-Group Out/Inner pooled**
The outer CV2 folds will be defined by the user using a vector defining groups (e.g., if there are different sites then this will test the ability of the classifiers that are trained with pooled site data to generalise to new sites).

**3 | Nested Leave-Group-Out**
Both the outer and inner cycles will be separated based on the grouping vector that the user enters (e.g., in the site example, this will mean that the models will be optimised to generalise across sites in the CV1 cycle and then applied to different sites in the CV2 cycle).

**4 | Outer Leave-Group-Out/Inner Leave-Group-In**
The models in the inner CV1 cycle will only be trained in one group prior to being applied to all other groups (e.g., this will test the robustness of models from single sites). See {numref}`fig:3.2.01_nested_cv` for more details and imagine that the folds are sites or diagnoses or any other grouping variable.
:::

### Repeated cross-valiation

To further avoid overfitting and increase the generalizability of the model predictions, NeuroMiner also allows the user to implement ’repeated’ cross-validation for each of the CV cycles. This involves shuffling the subjects prior to the definition of the folds so that there are different subjects in the test/training folds. This is usually done for both the CV2 and CV1 cycles.

A clear depiction of how repeated cross-validation works is difficult, but it is important to keep in-mind that there will be a lot of models produced under these schemes; e.g., if you have a standard 10x10 CV1/CV2 framework and you’re choosing the optimal model for each fold, then you’ll have 10,000 models. A way to understand how these are applied to an individual subject is to consider that all models in which that subject is not included in the CV1 training/testing are applied to them–because if they were included then this would be information leakage. On the surface, having 10,000 models is not so great for the interpretation or understanding of a phenomena under investigation. However, doing this can be really good for generalization when it’s unlikely that there is a singular model that can be found and we need something that practically works–e.g., for many complex problems in psychiatry. For an interesting discussion on this topic, see [Breiman et al., 2001](http://www.stat.uchicago.edu/~lekheng/courses/191f09/breiman.pdf).

---

### Define the number pf folds & permutations in CV1 & CV2

In the cross-validation menu ({numref}`fig:3.2.01_cv_menu`) you can define the number of permutations ("repeats") and folds for CV1 and CV2 respectively using the options 2 to 5 (note that these might change depending on the cross-validation framework you choose!).

#### Choosing the number of permutations
If you choose to use repeated cross-validation, the subjects are shuffled and new folds are created. To choose the number of permutations, try to find a balance between the computational time that your analysis is taking and the stability of the predictions from your modeling.

### Choosing the number of folds
The number of folds is usually based on the number of subjects you have and what kind of analyses you want to compute.

For an analytic example of when the number of CV1 folds is crucial for the outcome see the section on [ensemble generation strategies]{3.2.05_paramtemp_ensemble_generation_strategies} – e.g., when using wrappers that are optimizing features based on the subjects in the CV1 test folds.

:::{tip}
If you enter "-1" as the number of CV1 or CV2 folds, then NeuroMiner implements a **leave-one-out cross-validation** and the permutations option is not possible (because it doesn’t matter if you shuffle the data if every single subject is left out). The number of CV folds is usually based on the number of subjects that you have.
:::

---

### Equalize class sizes at the CV1 cycle by undersampling

Unbalanced group sizes (e.g., less people transitioning to illness than people who do not) pose a problem for support vector machines and other analysis techniques. Usually, the trained model shows a bias towards the majority group and you’ll get unbalanced predictions (e.g., your specificity will be high, but sensitivity low).

In NeuroMiner there are three ways to deal with this:
1. undersampling the majority group during training
2. weighting the SVM hyperplane
3. Oversampling using "Adaptive synthetic sampling"

If you enable udnersampling by choosing one of these approaches, the majority group will be undersampled to balance the groups in the CV1 folds.

After selecting the undersampling option (by entering "6" if your menu looks like in {numref}`fig:3.2.01_cv_menu`, the corresponding number can change though depending on chosen settings), two new lines will be added to your menu:

```{figure} Images/NM_cv_menu_equalize.png
---
name: fig:NM_cv_menu_equalize
alt: Neurominer cross-validation equalize classes screenshot
---
```
<!-- you will be prompted with will be prompted with this: ”Enable class size balancing at the CV1 level”, and enabling this option will lead to a prompt: ”Perform histogram equalization using the target label or some other covariate”, you can choose depending on which parameter you want to undersampling your algorithm.  -->

**7 | Define positive/negative ratio after equalization (1=>same amount of positive/negative cases)**: This is the ratio of the bigger to smaller class sample sizes. Equal class sizes can be selected by entering in ”1”. The default is "1.5" because otherwise the models may not generalize well.
<!-- If the dataset has any covariates, selecting ”Covariate” option will require you to select the covariates (integers or range). -->

**8 | Shuffle removed observations to CV1 test data**: The next option gives the user the option to add the excluded CV1 subjects in the training folds to the CV1 test folds. This option is potentially important for potential generalizability of the models to the CV2 folds (and ultimately to real-life).

---

## Build, load & save the cross-validation structure

**Build CV2/CV1 structure**
Once the options have been selected above, then the user needs to build the CV2/CV1 structure. This literally creates all the folds and indexes the subjects that are to be included in the folds. These can be found in the NM structure (i.e., in "NM.cv").

:::{important}
The user must select **Build the cross-validation structure** before
exiting the cross-validation menu. Otherwise, the cross-validation framework will not be saved
within the NM structure.
:::

**Load CV structure**
If you have saved the CV structure for your data with the subjects indexed into each fold/permutation, you can load it using this function. This can be useful if you establish a new analysis with new settings, but you want to load the CV structure that you made before.

**Save CV structure**
This gives the user the option to save the indexed CV structure
containing the folds and permutations for later use. It is saved in a .mat file.
