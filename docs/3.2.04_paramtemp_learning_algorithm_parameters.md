(3.2.04_paramtemp_learning_algorithm_parameters)=
# Learning algorithm parameters

Parameters for a learning algorithm (e.g., an SVM or SVR) determine the way that the model is fitted to the data. A common example is the ”C” or ”slack” parameter for soft margin SVM that defines the penalty associated with misclassifying individuals – i.e., whether the model does not allow any error within the CV1 folds and very tightly fits the decision boundary, or whether it allows errors in order to improve generalizability. Therefore, parameters such as these can greatly affect the model performance and it is common practice in machine learning to optimize the parameters for your specific problem and data.

NeuroMiner was developed in order to optimize performance across pre-defined parameter ranges within the nested cross-validation framework using a gridsearch, which protects against overfitting due to the application of the trained models to held-out data. Default parameter ranges are provided in NeuroMiner based on the literature and empirical testing, but we strongly recommend that the parameters are defined for your study and problem.

As previously stated, NeuroMiner uses a dynamic menu configuration that changes based on previous input. This is also true for the learning algorithm parameters, whereby the menu options will change based on what you have selected in the ”Classification algorithm” section.

Here we show an example for a RBF-Gaussian kernel Support Vector Machine classifier.

> 1 | Define Slack/Regularization parameter(s)
>
> 2 | [Define Kernel parameter(s)](https://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f410)
>
> 3 | [Define Weighting exponents](https://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f410)

Model selection regularization option controls the optimization module in NeuroMiner. When a selection of parameter(s) is necessary regularization might be advisable. If you answer yes to this question the following options will be available.

> 1 | Criterion for cross-parameter model selection
>
> 2 | Suboption 1 dependent on (a)
>
> 3 | Suboption 2 dependent on (a)
>
> 4 | Specify cross-parameter model selection process.

**4 | Specify cross-parameter model selection process**

This option controls the number of models (parameters combination) selected. One optimal model based on the criteria and regularization defined previously or an ensemble of the top performing models
You are given the possibility to select between: (1) select a single optimum parameter node which returns one optimal model based on the criteria, (2) generate cross-node ensemble by aggregating base learners above a predefined percentile which results in an ensemble of the top performing models or (3) automatically determine optimal percentile for optimum cross-node ensemble performance which depends on the regularization that is explained above.


<!--

As previously stated, NeuroMiner uses a dynamic menu configuration that
changes based on previous input. This is no different for the learning
algorithm parameters, whereby the menu options will change based on what
you have selected in the \"Classification algorithm\" section.

Here we show an example for a RBF-Gaussian kernel Support Vector Machine
**classifier**.

1.  [Define Slack/Regularization
    parameter(s)](https://en.wikipedia.org/wiki/Support_vector_machine#Parameter_selection)

2.  [Define Kernel
    parameter(s))](https://en.wikipedia.org/wiki/Support_vector_machine#Parameter_selection)

3.  [Define Weighting
    exponents](https://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f410)

4.  Enable regularization of model selection

Model selection regularization option controls the optimization module
in Neurominer. When parameter(s) selection is necessary regularization
might be advisable. If you answer yes to this question the following
options will be available.

1.  Criterion for cross-parameter model selection

2.  Suboption 1 dependent on (a)

3.  Suboption 2 dependent on (a)

4.  Specify cross-parameter model selection process. This option
    controls the number of models (parameters combination) selected. One
    optimal model based on the criteria and regularization defined
    previously or an ensemble of the top performing models -->
